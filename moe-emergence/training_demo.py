"""
Automatically generated by Claude 4.5 Opus. I have not verified this code.

MoE TRAINING DEMONSTRATION

This file demonstrates how to train an MoE layer with all components:
  • Load balancing loss (prevents expert collapse)
  • Z-loss (stabilizes router logits)
  • Noisy routing with annealing (helps experts specialize)
  • Straight-through estimator (for top-1 gradient flow)
"""

import torch
import torch.nn as nn

from moe import MoE


def create_fake_data(batch_size: int, seq_len: int, hidden_dim: int, vocab_size: int):
    """
    Creates fake input and target data for demonstration.
    In real training, this would come from your dataset.
    """
    # Random "hidden states" (normally these come from embeddings + attention)
    x = torch.randn(batch_size, seq_len, hidden_dim)

    # Random target tokens (for computing LM loss)
    targets = torch.randint(0, vocab_size, (batch_size, seq_len))

    return x, targets


def compute_fake_lm_loss(
    moe_output: torch.Tensor, targets: torch.Tensor, vocab_size: int
):
    """
    Fake LM loss for demonstration.

    In real training:
        1. MoE output goes through more layers (attention, etc.)
        2. Final layer projects to vocab_size
        3. Cross-entropy loss against target tokens

    Here we just project directly to vocab and compute loss.
    """
    batch, seq_len, hidden_dim = moe_output.shape

    # Fake "language model head" that projects to vocabulary
    lm_head = nn.Linear(hidden_dim, vocab_size)

    # Get logits and compute cross-entropy
    logits = lm_head(moe_output)  # [batch, seq_len, vocab_size]
    logits_flat = logits.view(-1, vocab_size)  # [batch * seq_len, vocab_size]
    targets_flat = targets.view(-1)  # [batch * seq_len]

    lm_loss = nn.functional.cross_entropy(logits_flat, targets_flat)
    return lm_loss


def print_expert_utilization(
    moe: MoE, router_probs: torch.Tensor, topk_indices: torch.Tensor
):
    """Shows which experts are being used and how much."""
    n_tokens = topk_indices.shape[0]

    print("  Expert utilization:")
    for i in range(moe.n_experts):
        count = (topk_indices == i).sum().item()
        fraction = count / (n_tokens * moe.topk)
        bar = "█" * int(fraction * 20)
        print(f"    Expert {i}: {bar:<20} {fraction:.1%}")


def main():
    # ═══════════════════════════════════════════════════════════════════════
    # SETUP
    # ═══════════════════════════════════════════════════════════════════════

    torch.manual_seed(42)

    # Model configuration
    hidden_dim = 64
    ffn_dim = 128
    n_experts = 4
    topk = 1  # Using top-1 to demonstrate STE gradient flow
    noise_std = 0.1

    # Training configuration
    batch_size = 4
    seq_len = 16
    vocab_size = 100
    total_steps = 20
    anneal_fraction = 0.5  # Anneal noise over first 50% of training

    # Loss coefficients (these are hyperparameters you'd tune)
    balance_loss_coef = 0.01  # α - how much to weight load balancing
    z_loss_coef = 0.001  # β - how much to weight z-loss

    print("═" * 70)
    print("MoE TRAINING DEMONSTRATION")
    print("═" * 70)
    print("\nConfiguration:")
    print(f"  Experts: {n_experts}, Top-k: {topk}")
    print(f"  Noise std: {noise_std}, Anneal over: {anneal_fraction:.0%} of training")
    print(f"  Loss coefficients: α={balance_loss_coef}, β={z_loss_coef}")

    # ═══════════════════════════════════════════════════════════════════════
    # CREATE MODEL AND OPTIMIZER
    # ═══════════════════════════════════════════════════════════════════════

    moe = MoE(
        hidden_dim=hidden_dim,
        ffn_dim=ffn_dim,
        n_experts=n_experts,
        topk=topk,
        noise_std=noise_std,
    )

    # Configure noise annealing
    # This will linearly reduce noise to 0 over the first 50% of training
    moe.router.set_noise_annealing(
        total_steps=total_steps, anneal_fraction=anneal_fraction
    )

    optimizer = torch.optim.Adam(moe.parameters(), lr=1e-3)

    print("\nNoise annealing schedule:")
    print(
        f"  Steps 0-{int(total_steps * anneal_fraction)}: noise decays {noise_std} → 0"
    )
    print(
        f"  Steps {int(total_steps * anneal_fraction)}-{total_steps}: noise = 0 (exploitation)"
    )

    # ═══════════════════════════════════════════════════════════════════════
    # TRAINING LOOP
    # ═══════════════════════════════════════════════════════════════════════

    print("\n" + "─" * 70)
    print("TRAINING LOOP")
    print("─" * 70)

    for step in range(total_steps):
        # ───────────────────────────────────────────────────────────────────
        # Step 1: Get data
        # ───────────────────────────────────────────────────────────────────
        x, targets = create_fake_data(batch_size, seq_len, hidden_dim, vocab_size)

        # ───────────────────────────────────────────────────────────────────
        # Step 2: Forward pass through MoE
        # ───────────────────────────────────────────────────────────────────
        # Returns: hidden_states, balance_loss, z_loss
        moe_output, balance_loss, z_loss = moe(x)

        # ───────────────────────────────────────────────────────────────────
        # Step 3: Compute language modeling loss
        # ───────────────────────────────────────────────────────────────────
        # In real training, moe_output goes through more transformer layers
        # before computing LM loss. Here we fake it for demonstration.
        lm_loss = compute_fake_lm_loss(moe_output, targets, vocab_size)

        # ───────────────────────────────────────────────────────────────────
        # Step 4: Combine all losses
        # ───────────────────────────────────────────────────────────────────
        #
        # total_loss = lm_loss + α * balance_loss + β * z_loss
        #
        # Where:
        #   lm_loss: Main objective (predict next token)
        #   balance_loss: Auxiliary (prevent expert collapse)
        #   z_loss: Auxiliary (stabilize router logits)
        #
        total_loss = lm_loss + balance_loss_coef * balance_loss + z_loss_coef * z_loss

        # ───────────────────────────────────────────────────────────────────
        # Step 5: Backward pass
        # ───────────────────────────────────────────────────────────────────
        optimizer.zero_grad()
        total_loss.backward()

        # ───────────────────────────────────────────────────────────────────
        # Step 6: Update weights
        # ───────────────────────────────────────────────────────────────────
        optimizer.step()

        # ───────────────────────────────────────────────────────────────────
        # Step 7: Update router noise schedule (IMPORTANT!)
        # ───────────────────────────────────────────────────────────────────
        # This decrements the noise level according to the annealing schedule.
        # If you forget this, noise stays constant!
        moe.router.step()

        # ───────────────────────────────────────────────────────────────────
        # Logging
        # ───────────────────────────────────────────────────────────────────
        if step % 5 == 0 or step == total_steps - 1:
            # Calculate current noise level
            if moe.router.training_step < moe.router.anneal_steps:
                progress = (
                    moe.router.training_step.float() / moe.router.anneal_steps.float()
                )
                current_noise = noise_std * (1.0 - progress.item())
            else:
                current_noise = 0.0

            print(f"\nStep {step:3d}:")
            print("  Losses:")
            print(f"    LM loss:      {lm_loss.item():.4f}")
            print(f"    Balance loss: {balance_loss.item():.4f} (target: ~1.0)")
            print(f"    Z-loss:       {z_loss.item():.4f}")
            print(f"    Total loss:   {total_loss.item():.4f}")
            print("  Router:")
            print(f"    Current noise: {current_noise:.4f}")

            # Check router gradients (verifies STE is working for top-1)
            router_grad = moe.router.gate.weight.grad
            if router_grad is not None:
                grad_norm = router_grad.norm().item()
                print(
                    f"    Gradient norm: {grad_norm:.4f} (>0 means router is learning)"
                )

    # ═══════════════════════════════════════════════════════════════════════
    # FINAL ANALYSIS
    # ═══════════════════════════════════════════════════════════════════════

    print("\n" + "═" * 70)
    print("FINAL ANALYSIS")
    print("═" * 70)

    # Run one more forward pass to see final expert utilization
    moe.eval()  # Disable noise for evaluation
    with torch.no_grad():
        x, _ = create_fake_data(batch_size * 4, seq_len, hidden_dim, vocab_size)
        (
            topk_weights,
            topk_indices,
            router_probs,
            router_probs_clean,
            router_logits,
            entropy,
        ) = moe.router(x)
        # Note: using router_probs (post-noise) for utilization analysis matches training behavior

    print("\nFinal expert utilization (should be roughly balanced):")
    print_expert_utilization(moe, router_probs, topk_indices)

    # Check router logit magnitudes (z-loss should keep these reasonable)
    print("\nRouter logit statistics:")
    print(f"  Mean: {router_logits.mean().item():.4f}")
    print(f"  Std:  {router_logits.std().item():.4f}")
    print(f"  Min:  {router_logits.min().item():.4f}")
    print(f"  Max:  {router_logits.max().item():.4f}")

    print("\n" + "═" * 70)
    print("TRAINING COMPLETE!")
    print("═" * 70)

    # ═══════════════════════════════════════════════════════════════════════
    # SUMMARY OF WHAT HAPPENED
    # ═══════════════════════════════════════════════════════════════════════

    print(
        """
WHAT HAPPENED DURING TRAINING:
──────────────────────────────

1. FORWARD PASS:
   x → Router → (topk_weights, topk_indices) → Experts → weighted sum → output
              ↓
        Also returns: router_probs (for balance loss)
                      router_logits (for z-loss)

2. LOSS COMPUTATION:
   ┌─────────────┐   ┌───────────────┐   ┌────────┐
   │   LM Loss   │ + │ Balance Loss  │ + │ Z-Loss │ = Total Loss
   │  (main obj) │   │ (prevent      │   │ (stab- │
   │             │   │  collapse)    │   │ ility) │
   └─────────────┘   └───────────────┘   └────────┘

3. GRADIENT FLOW:
   For top-1 routing, the Straight-Through Estimator ensures:
   - Forward: weight = 1.0 (preserves scale)
   - Backward: gradient flows through soft probability (router learns)

4. NOISE ANNEALING:
   Steps 0-10:  High noise → exploration, experts try different tokens
   Steps 10-20: No noise → exploitation, router uses learned preferences

5. WHAT TO LOOK FOR:
   - Balance loss ≈ 1.0 (experts evenly used)
   - Z-loss stays bounded (no logit explosion)
   - Router gradient norm > 0 (router is learning)
   - Expert utilization roughly uniform (no collapse)
"""
    )


if __name__ == "__main__":
    main()
