# Bug Fix: gpt2_moe.py Gaps from GPT-5.2 Review

**Date:** 2025-12-23
**Commit:** `1f2b581`
**Reviewer:** GPT-5.2
**Verifier:** Claude Opus 4.5

---

## Summary

GPT-5.2 reviewed the codebase after the initial `moe.py` fixes (commit `eea9294`) and identified 4 issues where fixes weren't propagated to `gpt2_moe.py` or docstrings were stale.

All findings were validated as correct and have been fixed.

---

## Fixes Applied

### Fix 1: `collect_aux_outputs` missing `router_probs_clean` and `entropy`

**Severity:** Medium
**Location:** `gpt2_moe.py:328-354`

**Problem:**
The helper function extracted only 4 fields from `MoEWrapperOutput`, omitting `router_probs_clean` and `entropy` which are required per V3 spec for non-confounded entropy logging.

**Fix:**
Added both fields to the returned dict:

```python
{
    "layer_idx": layer_idx,
    "router_probs": moe.last_aux.router_probs,
    "router_probs_clean": moe.last_aux.router_probs_clean,  # NEW
    "router_logits": moe.last_aux.router_logits,
    "topk_indices": moe.last_aux.topk_indices,
    "topk_weights": moe.last_aux.topk_weights,
    "entropy": moe.last_aux.entropy,  # NEW
}
```

---

### Fix 2: `.view()` on potentially non-contiguous tensors

**Severity:** Low
**Location:** `gpt2_moe.py:125`

**Problem:**
The L1 fix from the original review was applied to `moe.py` but not to `gpt2_moe.py`. HuggingFace outputs could be non-contiguous in edge cases.

**Fix:**

```python
# Before
hidden_flat = hidden_states.view(-1, hidden_dim)

# After
hidden_flat = hidden_states.reshape(-1, hidden_dim)
```

---

### Fix 3: Stale Router docstring example

**Severity:** Low
**Location:** `moe.py:65-71`

**Problem:**
The H1 fix added `router_probs_clean` and `entropy` to `RouterOutput` (6 fields total), but the docstring example still showed unpacking 4 values.

**Fix:**
Updated example to use named access pattern:

```python
>>> out = router(x)  # RouterOutput with 6 fields
>>> out.topk_weights.shape  # [20, 2]
>>> out.router_probs_clean.shape  # [20, 8] - for entropy logging
>>> out.entropy.shape  # [20] - per-token routing entropy
```

---

### Fix 4: Imprecise load-balance docstring in `gpt2_moe.py`

**Severity:** Low
**Location:** `gpt2_moe.py:204-206`

**Problem:**
The L3 fix was applied to `moe.py` but not to the standalone `compute_load_balance_loss` function in `gpt2_moe.py`. Docstring said "fraction of tokens" which is imprecise for top-k > 1.

**Fix:**

```python
# Before
- f_i: fraction of tokens ROUTED to expert i (hard assignment)

# After
- f_i: fraction of total routing assignments to expert i (for top-k routing,
      each token contributes k assignments, so sum of f_i = 1.0)
```

---

## Verification

- [x] `ruff check` passes
- [x] `training_demo.py` runs successfully
- [x] All 4 fixes validated against original GPT-5.2 findings

---

## Lesson Learned

When fixing issues in duplicated code paths (e.g., standalone functions vs. class methods), ensure fixes are propagated to ALL locations. The `compute_load_balance_loss` function exists in both `moe.py` (as a method) and `gpt2_moe.py` (as a standalone function).

---

## References

- Previous review: `002-2025-12-23-moe-py-fix.md`
- V3 Design Doc: `project-design/MOE-PROJECT-DESIGN-V3.md`
