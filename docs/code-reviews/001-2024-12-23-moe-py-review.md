# Code Review: moe.py

**Date:** 2024-12-23
**Reviewers:** Claude Opus 4.5, GPT-5.2
**Commit Reviewed:** `31252b6ddc4a73c8776ffdf8ca53fc79988f35c2`
**Commit Message:** "moe code done, todo: moe wrapper with gpt2 ffn deepcopy"

---

## Summary

Independent code review of `moe-emergence/moe.py` conducted by two models (Claude Opus 4.5 and GPT-5.2) against the V3 design specification (`project-design/MOE-PROJECT-DESIGN-V3.md`). Both reviewers converged on the same findings.

**Overall Assessment:** Core MoE logic (STE, load balancing, z-loss) is correctly implemented. Several gaps exist between the current implementation and V3 spec that must be addressed before training.

---

## Findings

### HIGH Severity

#### H1: Missing `router_probs_clean` and `entropy` in RouterOutput

**Location:** `moe.py:8-12`, `moe.py:186-261`

**Problem:**
The V3 design (lines 391-401) requires the Router to return:

- `router_probs_clean`: Clean (pre-noise) probabilities for entropy logging
- `entropy`: Pre-computed entropy for analysis

Current `RouterOutput`:

```python
class RouterOutput(NamedTuple):
    topk_weights: Tensor
    topk_indices: Tensor
    router_probs: Tensor      # noisy during annealing
    router_logits: Tensor     # raw logits
```

**Impact:**
Research goal is to demonstrate expert specialization emergence. If entropy is computed from noisy `router_probs`, measurements will confound "actual router confidence" with "noise level at training step." As noise anneals to zero, entropy will decrease even if router learned nothing - this invalidates the specialization analysis.

**V3 Reference:** Lines 150-154, 391-401

**Fix Required:**

```python
class RouterOutput(NamedTuple):
    topk_weights: Tensor
    topk_indices: Tensor
    router_probs: Tensor        # post-noise, for load balancing
    router_probs_clean: Tensor  # pre-noise, for entropy logging
    router_logits: Tensor       # raw logits, for z-loss
    entropy: Tensor             # pre-computed from clean probs
```

---

### MEDIUM Severity

#### M1: Router Noise Disabled Unless Explicitly Enabled

**Location:** `moe.py:100`, `moe.py:173-177`

**Problem:**
`anneal_steps` is initialized to 0:

```python
self.register_buffer("anneal_steps", torch.tensor(0, dtype=torch.long))
```

The noise condition:

```python
if (
    self.training
    and self.anneal_steps > 0      # Always False until set_noise_annealing() called
    and self.training_step < self.anneal_steps
):
```

The docstring at line 44 states:

> "noise_std: Standard deviation of noise to add to logits (default 0.1)"

This implies noise is active by default, but it isn't.

**Impact:**
If user forgets to call `set_noise_annealing()`, symmetry breaking via noisy routing won't happen. Experts may fail to diverge, or diverge slower than expected.

**Fix Options:**

1. **Option A (Recommended):** Make noise opt-out. Initialize with reasonable default anneal_steps, add `disable_noise()` method.
2. **Option B:** Fix docstring to clarify noise requires explicit activation via `set_noise_annealing()`.

**Both Reviewers Agree:** Yes (GPT-5.2 Medium, Claude Medium)

---

#### M2: STE Applied Unconditionally (Minor)

**Location:** `moe.py:252-254`

**Problem:**
V3 specifies STE should only be applied during training:

```python
if self.training:
    weights = hard + (soft - soft.detach())  # STE
else:
    weights = hard  # Inference: deterministic
```

Current code applies STE unconditionally.

**Impact:**
Functionally identical (forward value is 1.0 either way). Minor unnecessary computation during inference.

**Fix:** Add training mode check.

---

### LOW Severity

#### L1: `.view()` on Potentially Non-Contiguous Tensors

**Location:** `moe.py:133`, `moe.py:374`

**Problem:**

```python
x_flat = x.view(-1, hidden_dim)
```

`.view()` requires contiguous memory. Non-contiguous tensors (from slicing, transposing) will cause runtime error.

**Fix:** Use `.reshape()` instead (handles non-contiguous automatically).

**Both Reviewers Agree:** Yes

---

#### L2: Router Assumes 3D Input

**Location:** `moe.py:132`

**Problem:**

```python
_, _, hidden_dim = x.shape
```

Fails if 2D input `[tokens, hidden]` is passed.

**Impact:**
Integration friction. V3 wrapper flattens to 2D before calling router, so supporting 2D would reduce errors.

**Fix Options:**

1. Add input validation with clear error message
2. Support both 2D and 3D inputs

**Both Reviewers Agree:** Yes

---

#### L3: Load Balancing Docstring Wording

**Location:** `moe.py:401`

**Problem:**
Docstring says "f_i: fraction of tokens ROUTED to expert i"

For top-k > 1, what's computed is "fraction of assignments" (each token contributes k assignments). Mathematically correct per Switch Transformer, but wording is imprecise.

**Fix:** Update docstring to clarify.

**Both Reviewers Agree:** Yes

---

## Verified as Correct

The following implementations were verified as correctly matching V3 spec:

| Component                            | Location       | Verification                                                         |
| ------------------------------------ | -------------- | -------------------------------------------------------------------- |
| STE for top-1                        | moe.py:194-254 | Forward=1.0 (warm-start preserved), backward flows through soft prob |
| Load balancing loss                  | moe.py:394-460 | Matches Switch Transformer: `n_experts * Σ(f_i * P_i)`               |
| Z-loss                               | moe.py:462-518 | Correct: `mean(logsumexp(logits)²)`                                  |
| Using noisy probs for load balancing | moe.py:390     | V3 lines 428-431 confirm this is correct                             |
| Expert dispatch loop                 | moe.py:381-388 | Acknowledged bottleneck, fine at this scale                          |

---

## Next Steps

### Immediate (Before Training) - ✅ COMPLETED

All fixes applied in commit `eea9294`. See `002-2024-12-23-moe-py-fix.md` for details.

- [x] **Fix H1:** Add `router_probs_clean` and `entropy` to RouterOutput
- [x] **Fix M1:** Fixed docstring to clarify opt-in behavior
- [x] **Fix M2:** STE only applied during training
- [x] **Fix L1:** Replace `.view()` with `.reshape()`
- [x] **Fix L3:** Fixed load balancing docstring wording

### Before Analysis Phase

- [ ] Verify entropy logging uses clean probs
- [ ] Add per-layer entropy tracking in training loop

### Optional Improvements

- [ ] Support 2D input in Router
- [x] ~~Add training mode check for STE~~ (done in M2)
- [x] ~~Clarify load balancing docstring~~ (done in L3)

---

## Appendix: Reviewer Agreement Matrix

| Finding ID | GPT-5.2 Severity | Claude Severity | Agreement                          |
| ---------- | ---------------- | --------------- | ---------------------------------- |
| H1         | Medium           | HIGH            | Agree on issue, differ on severity |
| M1         | Medium           | Medium          | Full agree                         |
| M2         | Not mentioned    | Medium (minor)  | Claude only                        |
| L1         | Low              | Low             | Full agree                         |
| L2         | Low              | Low             | Full agree                         |
| L3         | Low              | Low             | Full agree                         |

---

## References

- V3 Design Doc: `project-design/MOE-PROJECT-DESIGN-V3.md`
- GPT-5.2 Review: `MODELS-DEBATE.md`
- Switch Transformer: Fedus et al., 2021
- ST-MoE (z-loss): Zoph et al., 2022
