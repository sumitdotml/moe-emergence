"""
##########################################################################################
#### NOTE: This code was generated by Claude Opus 4.5 for quick verification of       ####
#### dataset formatting artifacts. I have not pragmatically verified the code myself. ####
##########################################################################################

Verify datasets for formatting artifacts that could cause data leakage.

Inspired by MMLU-Pro issue where leading whitespace in correct answers
created an exploitable signal. See:
https://huggingface.co/datasets/TIGER-Lab/MMLU-Pro/discussions/41

Checks:
1. Leading/trailing whitespace patterns
2. Invisible characters (zero-width spaces, BOM, etc.)
3. Unusual Unicode (non-ASCII patterns)
4. Consistent formatting artifacts across samples

Usage:
    uv run python moe_emergence/verify_formatting.py
    uv run python moe_emergence/verify_formatting.py --samples 500

Check docs/decisions/011-formatting-artifact-verification.md for more details.
"""

import argparse
from collections import Counter
from pathlib import Path
import unicodedata

REPO_ROOT = Path(__file__).parent.parent
CACHE_DIR = REPO_ROOT / ".cache"

# invisible/problematic unicode characters to check for
INVISIBLE_CHARS = {
    "\u200b": "ZERO WIDTH SPACE",
    "\u200c": "ZERO WIDTH NON-JOINER",
    "\u200d": "ZERO WIDTH JOINER",
    "\ufeff": "BOM / ZERO WIDTH NO-BREAK SPACE",
    "\u00a0": "NON-BREAKING SPACE",
    "\u2028": "LINE SEPARATOR",
    "\u2029": "PARAGRAPH SEPARATOR",
    "\u202a": "LEFT-TO-RIGHT EMBEDDING",
    "\u202b": "RIGHT-TO-LEFT EMBEDDING",
    "\u202c": "POP DIRECTIONAL FORMATTING",
    "\u202d": "LEFT-TO-RIGHT OVERRIDE",
    "\u202e": "RIGHT-TO-LEFT OVERRIDE",
    "\u2060": "WORD JOINER",
    "\u2061": "FUNCTION APPLICATION",
    "\u2062": "INVISIBLE TIMES",
    "\u2063": "INVISIBLE SEPARATOR",
    "\u2064": "INVISIBLE PLUS",
    "\u180e": "MONGOLIAN VOWEL SEPARATOR",
    "\u034f": "COMBINING GRAPHEME JOINER",
}


def check_whitespace(texts: list[str], domain: str) -> dict:
    """Check for leading/trailing whitespace anomalies."""
    results = {
        "leading_space": 0,
        "leading_tab": 0,
        "leading_newline": 0,
        "trailing_space": 0,
        "trailing_tab": 0,
        "trailing_newline": 0,
        "leading_whitespace_chars": Counter(),
        "trailing_whitespace_chars": Counter(),
    }

    for text in texts:
        if not text:
            continue

        # leading whitespace
        if text[0] == " ":
            results["leading_space"] += 1
        if text[0] == "\t":
            results["leading_tab"] += 1
        if text[0] == "\n":
            results["leading_newline"] += 1

        # tracking first char if whitespace
        if text[0].isspace():
            results["leading_whitespace_chars"][repr(text[0])] += 1

        # trailing whitespace
        if text[-1] == " ":
            results["trailing_space"] += 1
        if text[-1] == "\t":
            results["trailing_tab"] += 1
        if text[-1] == "\n":
            results["trailing_newline"] += 1

        # tracking last char if whitespace
        if text[-1].isspace():
            results["trailing_whitespace_chars"][repr(text[-1])] += 1

    total = len(texts)
    print(f"\n[{domain}] Whitespace Analysis ({total} samples):")
    print(
        f"  Leading space:   {results['leading_space']:>5} ({100 * results['leading_space'] / total:.1f}%)"
    )
    print(
        f"  Leading tab:     {results['leading_tab']:>5} ({100 * results['leading_tab'] / total:.1f}%)"
    )
    print(
        f"  Leading newline: {results['leading_newline']:>5} ({100 * results['leading_newline'] / total:.1f}%)"
    )
    print(
        f"  Trailing space:  {results['trailing_space']:>5} ({100 * results['trailing_space'] / total:.1f}%)"
    )
    print(
        f"  Trailing tab:    {results['trailing_tab']:>5} ({100 * results['trailing_tab'] / total:.1f}%)"
    )
    print(
        f"  Trailing newline:{results['trailing_newline']:>5} ({100 * results['trailing_newline'] / total:.1f}%)"
    )

    if results["leading_whitespace_chars"]:
        print(
            f"  Leading whitespace types: {dict(results['leading_whitespace_chars'])}"
        )
    if results["trailing_whitespace_chars"]:
        print(
            f"  Trailing whitespace types: {dict(results['trailing_whitespace_chars'])}"
        )

    return results


def check_invisible_chars(texts: list[str], domain: str) -> dict:
    """Check for invisible Unicode characters."""
    results = {char: 0 for char in INVISIBLE_CHARS}
    texts_with_invisible = 0

    for text in texts:
        found_any = False
        for char in INVISIBLE_CHARS:
            if char in text:
                results[char] += text.count(char)
                found_any = True
        if found_any:
            texts_with_invisible += 1

    total = len(texts)
    print(f"\n[{domain}] Invisible Character Analysis ({total} samples):")
    print(
        f"  Texts with invisible chars: {texts_with_invisible} ({100 * texts_with_invisible / total:.1f}%)"
    )

    found_any = False
    for char, count in results.items():
        if count > 0:
            found_any = True
            name = INVISIBLE_CHARS[char]
            print(f"  {name}: {count} occurrences")

    if not found_any:
        print("  [OK] No invisible characters found")

    return results


def check_unicode_categories(texts: list[str], domain: str) -> dict:
    """Analyze Unicode character categories in the dataset."""
    category_counts = Counter()
    non_ascii_chars = Counter()

    for text in texts:
        for char in text:
            cat = unicodedata.category(char)
            category_counts[cat] += 1
            if ord(char) > 127:
                non_ascii_chars[char] += 1

    total_chars = sum(category_counts.values())
    print(f"\n[{domain}] Unicode Category Analysis ({total_chars:,} total chars):")

    # group by major category
    major_cats = Counter()
    for cat, count in category_counts.items():
        major_cats[cat[0]] += count

    cat_names = {
        "L": "Letter",
        "M": "Mark",
        "N": "Number",
        "P": "Punctuation",
        "S": "Symbol",
        "Z": "Separator",
        "C": "Control/Other",
    }

    for cat, count in sorted(major_cats.items(), key=lambda x: -x[1]):
        name = cat_names.get(cat, cat)
        print(f"  {name} ({cat}): {count:>10,} ({100 * count / total_chars:.2f}%)")

    # showing top non-ascii characters
    if non_ascii_chars:
        print("\n  Top 15 non-ASCII characters:")
        for char, count in non_ascii_chars.most_common(15):
            try:
                name = unicodedata.name(char, "UNKNOWN")
            except ValueError:
                name = "UNKNOWN"
            print(f"    U+{ord(char):04X} {repr(char):>6} ({count:>5}x): {name}")

    return {"categories": dict(category_counts), "non_ascii": dict(non_ascii_chars)}


def check_first_chars(texts: list[str], domain: str) -> dict:
    """Analyze distribution of first characters (like MMLU-Pro issue)."""
    first_chars = Counter()
    for text in texts:
        if text:
            first_chars[text[0]] += 1

    total = len(texts)
    print(f"\n[{domain}] First Character Distribution ({total} samples):")

    # showing top 10
    for char, count in first_chars.most_common(10):
        pct = 100 * count / total
        if char.isspace():
            display = repr(char)
        elif char.isalnum():
            display = char
        else:
            display = f"{char} (U+{ord(char):04X})"
        print(f"  {display:>10}: {count:>5} ({pct:.1f}%)")

    # flagging if any single char dominates (>50%)
    top_char, top_count = first_chars.most_common(1)[0]
    if top_count / total > 0.5:
        print(
            f"  [WARNING] Single character dominates first position: {repr(top_char)}"
        )

    return dict(first_chars)


def check_patterns(texts: list[str], domain: str) -> dict:
    """Check for suspicious repeating patterns."""
    results = {
        "starts_with_number": 0,
        "starts_with_letter": 0,
        "starts_with_punct": 0,
        "common_prefixes": Counter(),
    }

    for text in texts:
        if not text:
            continue

        if text[0].isdigit():
            results["starts_with_number"] += 1
        elif text[0].isalpha():
            results["starts_with_letter"] += 1
        elif text[0] in "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~":
            results["starts_with_punct"] += 1

        prefix = text[:20]
        results["common_prefixes"][prefix] += 1

    total = len(texts)
    print(f"\n[{domain}] Pattern Analysis ({total} samples):")
    print(
        f"  Starts with number: {results['starts_with_number']:>5} ({100 * results['starts_with_number'] / total:.1f}%)"
    )
    print(
        f"  Starts with letter: {results['starts_with_letter']:>5} ({100 * results['starts_with_letter'] / total:.1f}%)"
    )
    print(
        f"  Starts with punct:  {results['starts_with_punct']:>5} ({100 * results['starts_with_punct'] / total:.1f}%)"
    )

    # showing most common prefixes if any repeat significantly
    common = [(p, c) for p, c in results["common_prefixes"].items() if c > 1]
    if common:
        common.sort(key=lambda x: -x[1])
        print("\n  Top repeated prefixes (first 20 chars):")
        for prefix, count in common[:5]:
            print(f"    {count:>3}x: {repr(prefix[:30])}")

    return results


def load_sample_data(n_samples: int = 200):
    """Load sample data from each domain."""
    from moe_emergence.data import load_code_data, load_math_data, load_prose_data

    print(f"Loading {n_samples} samples from each domain...")

    # small size to load quickly
    size_mb = max(0.5, n_samples * 0.005)  # rough estimate

    code_texts, _ = load_code_data(max_size_mb=size_mb)
    math_texts, _ = load_math_data(max_size_mb=size_mb)
    prose_texts, _ = load_prose_data(max_size_mb=size_mb)

    # limiting to requested samples
    code_texts = code_texts[:n_samples]
    math_texts = math_texts[:n_samples]
    prose_texts = prose_texts[:n_samples]

    print(f"  Code:  {len(code_texts)} samples")
    print(f"  Math:  {len(math_texts)} samples")
    print(f"  Prose: {len(prose_texts)} samples")

    return code_texts, math_texts, prose_texts


def main():
    parser = argparse.ArgumentParser(
        description="Check datasets for formatting artifacts"
    )
    parser.add_argument(
        "--samples", type=int, default=200, help="Samples per domain (default: 200)"
    )
    args = parser.parse_args()

    print("=" * 70)
    print("Dataset Formatting Artifact Verification")
    print("=" * 70)

    code_texts, math_texts, prose_texts = load_sample_data(args.samples)

    issues_found = []

    for domain, texts in [
        ("CODE", code_texts),
        ("MATH", math_texts),
        ("PROSE", prose_texts),
    ]:
        print(f"\n{'=' * 70}")
        print(f"Analyzing {domain} domain")
        print("=" * 70)

        ws_results = check_whitespace(texts, domain)
        inv_results = check_invisible_chars(texts, domain)
        check_unicode_categories(texts, domain)
        check_first_chars(texts, domain)
        check_patterns(texts, domain)

        # flagging significant issues
        total = len(texts)

        # >10% leading whitespace is suspicious
        leading_ws = ws_results["leading_space"] + ws_results["leading_tab"]
        if leading_ws / total > 0.1:
            issues_found.append(
                f"{domain}: {100 * leading_ws / total:.1f}% have leading whitespace"
            )

        # any invisible characters is suspicious
        inv_count = sum(inv_results.values())
        if inv_count > 0:
            issues_found.append(f"{domain}: {inv_count} invisible characters found")

    print(f"\n{'=' * 70}")
    print("SUMMARY")
    print("=" * 70)

    if issues_found:
        print("\n[WARNING] Potential issues found:")
        for issue in issues_found:
            print(f"  - {issue}")
        print("\nThese may create exploitable signals for routing shortcuts.")
    else:
        print("\n[OK] No significant formatting artifacts detected.")
        print("Datasets appear clean for training.")


if __name__ == "__main__":
    main()
